import streamlit as st
import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
import yaml
import tempfile
import ultralytics
from ultralytics import YOLO
import pandas as pd
import torch
import mlflow


def choose_model():
    st.write("### Ch·ªçn m√¥ h√¨nh YOLOv8")

    model_option = st.selectbox(
        "Ch·ªçn model",
        options=["YOLOv8n", "YOLOv8s", "YOLOv8m"],
        index=0
    )

    # Map model option to path
    model_paths = {
        "YOLOv8n": "yolov8n.pt",
        "YOLOv8s": "yolov8s.pt",
        "YOLOv8m": "yolov8m.pt",
    }

    model_path = model_paths[model_option]

    return model_path, model_option


def train_model():
    st.header("Training Module")
    # Ch·ªçn model
    model_path, model_name = choose_model()

    # Upload data config YAML
    st.write("#### C·∫•u h√¨nh d·ªØ li·ªáu (data.yaml)")
    yaml_file = st.file_uploader("Upload file data.yaml", type=['yaml','yml'])
    
    # Th√™m input cho ƒë∆∞·ªùng d·∫´n g·ªëc c·ªßa dataset
    st.write("#### ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c dataset")
    dataset_root = st.text_input("ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi ƒë·∫øn th∆∞ m·ª•c ch·ª©a dataset", 
                                value=os.path.abspath("./yolov8_dataset"))
    
    if not os.path.exists(dataset_root):
        st.warning(f"Th∆∞ m·ª•c '{dataset_root}' kh√¥ng t·ªìn t·∫°i. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.")

    # Tham s·ªë hu·∫•n luy·ªán
    st.write("#### Tham s·ªë hu·∫•n luy·ªán")
    epochs = st.slider("S·ªë epochs", min_value=1, max_value=100, value=30)
    imgsz = st.slider("K√≠ch th∆∞·ªõc ·∫£nh", min_value=320, max_value=1280, value=640)
    batch_size = st.slider("Batch size", min_value=1, max_value=64, value=16)
    lr = st.number_input("Learning rate", min_value=1e-6, max_value=1.0, value=0.01, format="%.6f")

    # Display selected parameters
    st.write("### Th√¥ng s·ªë ƒë√£ ch·ªçn:")

    # Create a dictionary of parameters
    params = {
        "Model": model_name,
        "Epochs": epochs,
        "Image Size": imgsz,
        "Batch Size": batch_size,
        "Learning Rate": lr,
        "Dataset Path": dataset_root
    }

    params_df = pd.DataFrame(params.items(), columns=["Tham s·ªë", "Gi√° tr·ªã"])
    st.table(params_df)

    if yaml_file is not None:
        # ƒê·ªçc n·ªôi dung YAML
        yaml_content = yaml.safe_load(yaml_file.read())
        
        # Ki·ªÉm tra xem YAML c√≥ ch·ª©a c√°c kh√≥a b·∫Øt bu·ªôc kh√¥ng
        required_keys = ['train', 'val']
        missing_keys = [key for key in required_keys if key not in yaml_content]
        
        if missing_keys:
            st.error(f"File YAML thi·∫øu c√°c kh√≥a b·∫Øt bu·ªôc: {', '.join(missing_keys)}. Vui l√≤ng ƒë·∫£m b·∫£o file YAML c√≥ c·∫£ 'train' v√† 'val'.")
            
            # Hi·ªÉn th·ªã n·ªôi dung YAML hi·ªán t·∫°i
            st.write("N·ªôi dung YAML hi·ªán t·∫°i:")
            st.code(yaml.dump(yaml_content), language="yaml")
            
            # Cung c·∫•p m·∫´u YAML ƒë√∫ng
            st.write("M·∫´u YAML ƒë√∫ng:")
            sample_yaml = {
                'path': dataset_root,
                'train': 'train/images',
                'val': 'val/images',
                'test': 'test/images',
                'names': {
                    0: 'auto',
                    1: 'bicycle',
                    # ... th√™m c√°c l·ªõp kh√°c
                }
            }
            st.code(yaml.dump(sample_yaml), language="yaml")
            
            return
        
        # C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi trong YAML
        yaml_content['path'] = dataset_root
        
        # Ki·ªÉm tra xem c√°c th∆∞ m·ª•c c√≥ t·ªìn t·∫°i kh√¥ng
        train_path = os.path.join(dataset_root, yaml_content['train'])
        val_path = os.path.join(dataset_root, yaml_content['val'])
        
        if not os.path.exists(train_path):
            st.error(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c train: {train_path}")
            return
            
        if not os.path.exists(val_path):
            st.error(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c validation: {val_path}")
            return
            
        # Hi·ªÉn th·ªã th√¥ng tin v·ªÅ ƒë∆∞·ªùng d·∫´n
        st.write("#### Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n dataset:")
        st.write(f"- Th∆∞ m·ª•c g·ªëc: {dataset_root} - T·ªìn t·∫°i: {os.path.exists(dataset_root)}")
        st.write(f"- Th∆∞ m·ª•c train: {train_path} - T·ªìn t·∫°i: {os.path.exists(train_path)}")
        st.write(f"- Th∆∞ m·ª•c validation: {val_path} - T·ªìn t·∫°i: {os.path.exists(val_path)}")
        
        # L∆∞u t·∫°m file YAML v·ªõi ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
        temp_yaml = tempfile.NamedTemporaryFile(delete=False, suffix='.yaml')
        with open(temp_yaml.name, 'w') as f:
            yaml.dump(yaml_content, f)
        data_cfg = temp_yaml.name
        
        # Hi·ªÉn th·ªã n·ªôi dung YAML ƒë√£ c·∫≠p nh·∫≠t
        st.write("#### N·ªôi dung YAML ƒë√£ c·∫≠p nh·∫≠t:")
        st.code(yaml.dump(yaml_content), language="yaml")

        if st.button("B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán üî•"):
            # B·∫Øt ƒë·∫ßu run MLflow
            with mlflow.start_run(run_name=model_name):
                try:
                    # Log parameters
                    mlflow.log_param("model", model_name)
                    mlflow.log_param("epochs", epochs)
                    mlflow.log_param("batch_size", batch_size)
                    mlflow.log_param("learning_rate", lr)
                    mlflow.log_param("data_config", os.path.basename(data_cfg))
                    mlflow.log_param("dataset_path", dataset_root)

                    # Kh·ªüi t·∫°o model
                    model = YOLO(model_path)
                    # Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£
                    save_dir = os.path.join('runs', 'train', model_name)
                    os.makedirs(save_dir, exist_ok=True)

                    # Hi·ªÉn th·ªã th√¥ng tin debug
                    st.write(f"ƒê∆∞·ªùng d·∫´n YAML: {data_cfg}")
                    with open(data_cfg, 'r') as f:
                        st.code(f.read(), language="yaml")

                    # Train
                    results = model.train(
                        data=data_cfg,
                        epochs=epochs,
                        imgsz=imgsz,
                        batch=batch_size,
                        lr0=lr,
                        project='runs/train',
                        name=model_name,
                        exist_ok=True
                    )

                    # Hi·ªÉn th·ªã k·∫øt qu·∫£ metrics
                    metrics = results.metrics
                    df = pd.DataFrame(metrics)
                    st.write("#### Training Metrics")
                    st.dataframe(df)

                    # Plot loss curves
                    if 'train/loss' in df.columns and 'val/loss' in df.columns:
                        fig, ax = plt.subplots()
                        ax.plot(df['epoch'], df['train/loss'], label='Train Loss')
                        ax.plot(df['epoch'], df['val/loss'], label='Val Loss')
                        ax.set_xlabel('Epoch')
                        ax.set_ylabel('Loss')
                        ax.legend()
                        st.pyplot(fig)

                    # Log metrics per epoch
                    for _, row in df.iterrows():
                        epoch = int(row['epoch'])
                        for col in df.columns:
                            if col != 'epoch':
                                mlflow.log_metric(col, float(row[col]), step=epoch)

                    # Log artifacts: plots and model
                    mlflow.log_artifacts(save_dir)
                    st.success("Training v√† logging MLflow ho√†n t·∫•t!")
                    
                except Exception as e:
                    st.error(f"L·ªói trong qu√° tr√¨nh hu·∫•n luy·ªán: {str(e)}")
                    st.exception(e)
                finally:
                    # X√≥a file t·∫°m
                    try:
                        os.unlink(data_cfg)
                    except:
                        pass
    else:
        st.info("Vui l√≤ng upload file data.yaml ƒë·ªÉ b·∫Øt ƒë·∫ßu hu·∫•n luy·ªán.")
        
        # # Hi·ªÉn th·ªã m·∫´u YAML
        # st.write("M·∫´u file data.yaml:")
        # sample_yaml = {
        #     'path': dataset_root if 'dataset_root' in locals() else './yolov8_dataset',
        #     'train': 'train/images',
        #     'val': 'val/images',
        #     'test': 'test/images',
        #     'names': {
        #         0: 'auto',
        #         1: 'bicycle',
        #         2: 'bus',
        #         3: 'car',
        #         4: 'tempo',
        #         5: 'tractor',
        #         6: 'two_wheelers',
        #         7: 'vehicle_truck'
        #     }
        # }
        # st.code(yaml.dump(sample_yaml), language="yaml")
        
        # # Th√™m t√πy ch·ªçn s·ª≠ d·ª•ng file YAML m·∫∑c ƒë·ªãnh
        # if st.button("S·ª≠ d·ª•ng file YAML m·∫∑c ƒë·ªãnh"):
        #     try:
        #         default_yaml_path = './yolov8_dataset/custom_dataset.yaml'
        #         if os.path.exists(default_yaml_path):
        #             with open(default_yaml_path, 'r') as f:
        #                 yaml_content = yaml.safe_load(f)
                        
        #             # C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
        #             if 'dataset_root' in locals():
        #                 yaml_content['path'] = dataset_root
                        
        #             st.success(f"ƒê√£ t·∫£i file YAML m·∫∑c ƒë·ªãnh t·ª´ {default_yaml_path}")
        #             st.write("N·ªôi dung ƒë√£ c·∫≠p nh·∫≠t:")
        #             st.code(yaml.dump(yaml_content), language="yaml")
        #             st.session_state.default_yaml_content = yaml_content
        #         else:
        #             st.error(f"Kh√¥ng t√¨m th·∫•y file YAML m·∫∑c ƒë·ªãnh t·∫°i {default_yaml_path}")
        #     except Exception as e:
        #         st.error(f"L·ªói khi t·∫£i file YAML m·∫∑c ƒë·ªãnh: {str(e)}")


