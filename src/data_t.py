import streamlit as st
import os
import yaml
import matplotlib.pyplot as plt
import random
from PIL import Image
import numpy as np
import matplotlib.patches as patches
import shutil
import gdown
import zipfile
import traceback


@st.cache_resource
def download_dataset_from_gdrive():
    try:
        st.info("Downloading dataset .zip from Google Drive...")

        # ‚úÖ File ID t·ª´ link c·ªßa b·∫°n
        file_id = "1KNgMtAIjTXRCx-Q-xLb_dida3YDnRG5K"
        zip_output = "yolov8_dataset.zip"

        # T·∫£i file .zip
        gdown.download(
            url=f"https://drive.google.com/uc?id={file_id}",
            output=zip_output,
            quiet=False
        )

        # Gi·∫£i n√©n
        with zipfile.ZipFile(zip_output, 'r') as zip_ref:
            zip_ref.extractall(".")

        # Ki·ªÉm tra k·∫øt qu·∫£
        yaml_path = "yolov8_dataset/custom_dataset.yaml"
        if os.path.exists(yaml_path):
            st.success("‚úÖ Dataset downloaded and extracted successfully!")
            
            # Verify that the directories contain files
            train_img_dir = "yolov8_dataset/train/images"
            train_label_dir = "yolov8_dataset/train/labels"
            
            if os.path.exists(train_img_dir) and os.path.exists(train_label_dir):
                img_count = len([f for f in os.listdir(train_img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])
                label_count = len([f for f in os.listdir(train_label_dir) if f.endswith('.txt')])
                
                st.info(f"Found {img_count} images and {label_count} label files in the training set.")
                
                if img_count == 0 or label_count == 0:
                    st.warning("Dataset directories are empty. The download may be incomplete.")
            
            # Clean up zip file
            try:
                os.remove(zip_output)
            except:
                pass
                
            return True
        else:
            st.error("‚ùå `custom_dataset.yaml` not found after extraction.")
            return False

    except Exception as e:
        st.error(f"Error downloading dataset: {str(e)}")
        st.code(traceback.format_exc())
        
        return False


def data_description():
    st.write("### T·∫≠p D·ªØ Li·ªáu Indian Vehicle Dataset")
    # Check if dataset exists locally
    yaml_path = './yolov8_dataset/custom_dataset.yaml'
    
    # Create base directory if it doesn't exist
    os.makedirs('./yolov8_dataset', exist_ok=True)
    
    if not os.path.exists(yaml_path):
        st.warning("Dataset not found locally.")
        
        # Create a more informative message for Streamlit web deployment
        st.info("""
        ### Dataset Options:
        
        1. ‚úÖ Download automatically from **Google Drive**
        2. üì• Or download manually from [Kaggle](https://www.kaggle.com/datasets/dataclusterlabs/indian-vehicle-dataset)
        
        **Note for Streamlit Web Deployment:** 
        If you're running this app on Streamlit Cloud, you'll need to download the dataset first.
        """)
        
        # Button to download from Google Drive
        if st.button("Download from Google Drive (.zip)"):
            success = download_dataset_from_gdrive()
            if success:
                st.rerun()  # Use st.rerun() instead of st.experimental_rerun()
            else:
                st.error("Failed to download dataset. Please try again or download manually.")
                return
        return
    
    # Display dataset information
    st.write("T·∫≠p d·ªØ li·ªáu n√†y ƒë∆∞·ª£c thu th√¢p b·ªüi DataCluster Labs. B·ªô d·ªØ li·ªáu n√†y l√† m·ªôt t·∫≠p h·ª£p g·ªìm h∆°n 50.000 h√¨nh ·∫£nh xe g·ªëc ƒë∆∞·ª£c ch·ª•p v√† thu th·∫≠p t·ª´ h∆°n 1000 khu v·ª±c th√†nh th·ªã v√† n√¥ng th√¥n, trong ƒë√≥ m·ªói h√¨nh ·∫£nh ƒë·ªÅu ƒë∆∞·ª£c c√°c chuy√™n gia v·ªÅ th·ªã gi√°c m√°y t√≠nh t·∫°i Datacluster Labs xem x√©t v√† x√°c minh th·ªß c√¥ng")

    # Display dataset information
    st.write("#### M·ªôt s·ªë th√¥ng tin v·ªÅ t·∫≠p d·ªØ li·ªáu:")
    st.write("- **K√≠ch th∆∞·ªõc t·∫≠p d·ªØ li·ªáu:** 50.000+ h√¨nh ·∫£nh")
    st.write("- **ƒê∆∞·ª£c ch·ª•p b·ªüi:** H∆°n 1000+ ng∆∞·ªùi ƒë√≥ng g√≥p c·ªông ƒë·ªìng")
    st.write("- **ƒê·ªô ph√¢n gi·∫£i:** 100% h√¨nh ·∫£nh l√† HD tr·ªü l√™n (1920x1080 tr·ªü l√™n)")
    st.write("- **ƒê·ªãa ƒëi·ªÉm:** Ch·ª•p ·∫£nh t·∫°i h∆°n 1000 th√†nh ph·ªë tr√™n kh·∫Øp ·∫§n ƒê·ªô")
    st.write("- **T√≠nh ƒëa d·∫°ng:** Nhi·ªÅu ƒëi·ªÅu ki·ªán √°nh s√°ng kh√°c nhau nh∆∞ ng√†y, ƒë√™m, kho·∫£ng c√°ch kh√°c nhau, ƒëi·ªÉm nh√¨n, ...")
    st.write(
        "- **Thi·∫øt b·ªã s·ª≠ d·ª•ng:** Ch·ª•p b·∫±ng ƒëi·ªán tho·∫°i di ƒë·ªông trong nƒÉm 2020-2022")
    st.write("- **C√¥ng d·ª•ng:** Ph√°t hi·ªán xe c·ªô, Ph√°t hi·ªán √¥ t√¥, Ph√°t hi·ªán xe x√¢y d·ª±ng, H·ªá th·ªëng t·ª± l√°i, ...")
    st.write("- **Nh·ªØng l·ªõp/ph∆∞∆°ng ti·ªán c√≥ trong t·∫≠p d·ªØ li·ªáu:** auto, bicycle, bus, car, tempo, tractor, two_wheelers, vehicle_truck")
    st.write("---")

    try:
        # Read YAML file
        with open(yaml_path, 'r') as f:
            dataset_config = yaml.safe_load(f)

        # Get class names
        class_names = list(dataset_config['names'].values())

        # Get base directory
        base_dir = os.path.dirname(yaml_path)

        # Construct paths properly
        train_path = dataset_config.get('train', 'train/images')
        val_path = dataset_config.get('val', 'val/images')
        test_path = dataset_config.get('test', 'test/images')

        # Remove any leading ./ from paths
        train_path = train_path[2:] if train_path.startswith(
            './') else train_path
        val_path = val_path[2:] if val_path.startswith('./') else val_path
        test_path = test_path[2:] if test_path.startswith('./') else test_path

        # Construct absolute paths
        train_dir = os.path.normpath(os.path.join(base_dir, train_path))
        val_dir = os.path.normpath(os.path.join(base_dir, val_path))
        test_dir = os.path.normpath(os.path.join(base_dir, test_path))

        # Create directories if they don't exist
        os.makedirs(train_dir, exist_ok=True)
        os.makedirs(val_dir, exist_ok=True)
        os.makedirs(test_dir, exist_ok=True)
        
        # Also create label directories
        train_label_dir = train_dir.replace('images', 'labels')
        val_label_dir = val_dir.replace('images', 'labels')
        test_label_dir = test_dir.replace('images', 'labels')
        
        os.makedirs(train_label_dir, exist_ok=True)
        os.makedirs(val_label_dir, exist_ok=True)
        os.makedirs(test_label_dir, exist_ok=True)

        # Check if directories have images
        train_images = [f for f in os.listdir(train_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]
        val_images = [f for f in os.listdir(val_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]
        test_images = [f for f in os.listdir(test_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]
        
        if not train_images or not val_images:
            st.warning("Dataset directories exist but no images found. You need to download the dataset.")
            
            if st.button("Download Dataset Now"):
                success = download_dataset_from_gdrive()
                if success:
                    st.rerun()  # Use st.rerun() instead of st.experimental_rerun()
                else:
                    st.error("Failed to download dataset automatically.")
                    st.info("Please download manually from [Kaggle](https://www.kaggle.com/datasets/dataclusterlabs/indian-vehicle-dataset) and extract to the yolov8_dataset directory.")
                return
            return

        # Count images in each set
        train_count = len(train_images)
        val_count = len(val_images)
        test_count = len(test_images)

        # Plot image distribution
        st.write("#### Ph√¢n b·ªë s·ªë l∆∞·ª£ng h√¨nh ·∫£nh trong c√°c t·∫≠p d·ªØ li·ªáu")
        fig, ax = plt.subplots(figsize=(8, 5))
        ax.bar(['Train', 'Val', 'Test'], [train_count, val_count, test_count],
               color=['blue', 'orange', 'green'])
        ax.set_title('Ph√¢n b·ªë s·ªë l∆∞·ª£ng h√¨nh ·∫£nh trong c√°c t·∫≠p d·ªØ li·ªáu')
        ax.set_xlabel('T·∫≠p d·ªØ li·ªáu')
        ax.set_ylabel('S·ªë l∆∞·ª£ng h√¨nh ·∫£nh')
        st.pyplot(fig)

        # Analyze dataset
        analyze_dataset(train_dir, class_names)
    except Exception as e:
        st.error(f"Error analyzing dataset: {str(e)}")
        st.info("Please make sure the dataset is properly downloaded and configured.")
        
        # Provide more detailed error information
        import traceback
        st.code(traceback.format_exc())
        
        # Offer to create a sample dataset structure
        if st.button("Create Sample Dataset Structure"):
            create_sample_dataset_structure()
            st.success("Created sample dataset structure. Please download the actual dataset files.")
            st.rerun()  # Use st.rerun() instead of st.experimental_rerun()


def create_sample_dataset_structure():
    """Create a sample dataset structure with the necessary directories"""
    try:
        # Create base directory
        os.makedirs('./yolov8_dataset', exist_ok=True)
        
        # Create subdirectories
        for split in ['train', 'val', 'test']:
            for subdir in ['images', 'labels']:
                os.makedirs(f'./yolov8_dataset/{split}/{subdir}', exist_ok=True)
        
        # Create a sample YAML file
        yaml_content = {
            'path': './yolov8_dataset',
            'train': 'train/images',
            'val': 'val/images',
            'test': 'test/images',
            'names': {
                0: 'auto',
                1: 'bicycle',
                2: 'bus',
                3: 'car',
                4: 'tempo',
                5: 'tractor',
                6: 'two_wheelers',
                7: 'vehicle_truck'
            }
        }
        
        with open('./yolov8_dataset/custom_dataset.yaml', 'w') as f:
            yaml.dump(yaml_content, f, default_flow_style=False)
            
        return True
    except Exception as e:
        st.error(f"Error creating sample dataset structure: {str(e)}")
        return False


# b. Ph√¢n t√≠ch ph√¢n b·ªë l·ªõp (n·∫øu c√≥ nh√£n)
def get_class_distribution(label_dir, class_names):
    class_counts = [0] * len(class_names)
    if not os.path.exists(label_dir):
        return class_counts

    try:
        for label_file in os.listdir(label_dir):
            if label_file.endswith('.txt'):
                with open(os.path.join(label_dir, label_file), 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if parts:  # Make sure line is not empty
                            try:
                                class_id = int(parts[0])
                                if 0 <= class_id < len(class_names):  # Check if class_id is valid
                                    class_counts[class_id] += 1
                            except (ValueError, IndexError):
                                # Skip invalid lines
                                continue
    except Exception as e:
        st.error(f"Error reading label files: {str(e)}")
        
    return class_counts


@st.cache_data
def analyze_dataset(train_dir, class_names):
    st.write("#### Ph√¢n b·ªë s·ªë l∆∞·ª£ng ƒë·ªëi t∆∞·ª£ng theo l·ªõp (T·∫≠p Train)")
    train_label_dir = train_dir.replace('images', 'labels')
    
    # Check if label directory exists and has files
    if not os.path.exists(train_label_dir) or not os.listdir(train_label_dir):
        st.warning(f"Kh√¥ng t√¨m th·∫•y nh√£n trong th∆∞ m·ª•c {train_label_dir}")
        st.info("Vui l√≤ng t·∫£i dataset ƒë·∫ßy ƒë·ªß bao g·ªìm c·∫£ th∆∞ m·ª•c labels.")
        
        # Create a placeholder chart with zeros
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.bar(class_names, [0] * len(class_names), color='purple')
        ax.set_title('Ph√¢n b·ªë s·ªë l∆∞·ª£ng ƒë·ªëi t∆∞·ª£ng theo l·ªõp (T·∫≠p Train) - Kh√¥ng c√≥ d·ªØ li·ªáu')
        ax.set_xlabel('L·ªõp')
        ax.set_ylabel('S·ªë l∆∞·ª£ng ƒë·ªëi t∆∞·ª£ng')
        plt.xticks(rotation=45)
        st.pyplot(fig)
    else:
        # Get class distribution
        train_class_counts = get_class_distribution(train_label_dir, class_names)
        
        # Plot class distribution
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.bar(class_names, train_class_counts, color='purple')
        ax.set_title('Ph√¢n b·ªë s·ªë l∆∞·ª£ng ƒë·ªëi t∆∞·ª£ng theo l·ªõp (T·∫≠p Train)')
        ax.set_xlabel('L·ªõp')
        ax.set_ylabel('S·ªë l∆∞·ª£ng ƒë·ªëi t∆∞·ª£ng')
        plt.xticks(rotation=45)
        st.pyplot(fig)

    # c. Hi·ªÉn th·ªã h√¨nh ·∫£nh m·∫´u v·ªõi bounding boxes (n·∫øu c√≥)
    display_sample_images_with_boxes(
        train_dir, train_label_dir, 'Train', class_names)

    # d. Ph√¢n t√≠ch ph√¢n b·ªë gi√° tr·ªã pixel (histogram)
    plot_pixel_distribution(train_dir)


@st.cache_data
def display_sample_images_with_boxes(image_dir, label_dir, title, class_names, num_samples=4):
    st.write("#### M·ªôt s·ªë m·∫´u v·ªõi bounding boxes")
    
    # Check if image directory exists and has images
    if not os.path.exists(image_dir):
        st.warning(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c h√¨nh ·∫£nh: {image_dir}")
        st.info("Vui l√≤ng t·∫£i dataset ƒë·∫ßy ƒë·ªß bao g·ªìm c·∫£ th∆∞ m·ª•c images.")
        return
    
    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]
    if not image_files:
        st.warning(f"Kh√¥ng t√¨m th·∫•y h√¨nh ·∫£nh trong th∆∞ m·ª•c {image_dir}")
        st.info("Vui l√≤ng t·∫£i dataset ƒë·∫ßy ƒë·ªß bao g·ªìm c·∫£ h√¨nh ·∫£nh.")
        
        # Offer to download the dataset
        if st.button("Download Dataset (from display_sample)"):
            success = download_dataset_from_gdrive()
            if success:
                st.rerun()
        return

    # Create a 2x2 grid of sample images
    sample_files = random.sample(image_files, min(num_samples, len(image_files)))
    fig, axes = plt.subplots(2, 2, figsize=(10, 10))
    axes = axes.flatten()

    for i, img_file in enumerate(sample_files):
        if i >= num_samples:
            break

        # ƒê·ªçc h√¨nh ·∫£nh
        img_path = os.path.join(image_dir, img_file)
        try:
            img = Image.open(img_path)
            img_array = np.array(img)
        except Exception as e:
            st.error(f"Error opening image {img_path}: {str(e)}")
            continue

        # ƒê·ªçc nh√£n (n·∫øu c√≥)
        label_path = os.path.join(label_dir, img_file.replace('.jpg', '.txt').replace('.png', '.txt').replace('.jpeg', '.txt'))
        boxes = []
        labels = []
        
        if os.path.exists(label_path):
            try:
                with open(label_path, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 5:  # Make sure we have all required values
                            class_id = int(parts[0])
                            center_x, center_y, width, height = map(float, parts[1:5])
                            
                            # Chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô YOLO sang t·ªça ƒë·ªô pixel
                            h, w = img_array.shape[:2]
                            x = (center_x - width / 2) * w
                            y = (center_y - height / 2) * h
                            box_w = width * w
                            box_h = height * h
                            
                            if 0 <= class_id < len(class_names):  # Check if class_id is valid
                                boxes.append([x, y, box_w, box_h])
                                labels.append(class_names[class_id])
            except Exception as e:
                st.error(f"Error reading label file {label_path}: {str(e)}")

        # V·∫Ω h√¨nh ·∫£nh v√† bounding boxes
        axes[i].imshow(img_array)
        for box, label in zip(boxes, labels):
            rect = patches.Rectangle(
                (box[0], box[1]), box[2], box[3], linewidth=2, edgecolor='r', facecolor='none')
            axes[i].add_patch(rect)
            axes[i].text(box[0], box[1], label, color='red',
                         fontsize=10, bbox=dict(facecolor='white', alpha=0.5))
        axes[i].set_title(f'{title} - M·∫´u {i+1}')
        axes[i].axis('off')

    plt.tight_layout()
    st.pyplot(fig)


@st.cache_data
def plot_pixel_distribution(image_dir):
    if not os.path.exists(image_dir):
        st.warning(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c h√¨nh ·∫£nh: {image_dir}")
        return
        
    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]
    if not image_files:
        st.warning(f"Kh√¥ng t√¨m th·∫•y h√¨nh ·∫£nh trong th∆∞ m·ª•c {image_dir}")
        return

    try:
        sample_img_path = os.path.join(image_dir, image_files[0])
        sample_img = Image.open(sample_img_path)
        img_array = np.array(sample_img)
        
        # Check if image is RGB (3 channels)
        if len(img_array.shape) < 3 or img_array.shape[2] < 3:
            st.warning(f"H√¨nh ·∫£nh {sample_img_path} kh√¥ng ph·∫£i ƒë·ªãnh d·∫°ng RGB")
            return
            
        fig, ax = plt.subplots(figsize=(10, 5))
        for i, color in enumerate(['red', 'green', 'blue']):
            ax.hist(img_array[:, :, i].ravel(), bins=256,
                    color=color, alpha=0.5, label=color.capitalize())
        ax.set_title('Ph√¢n b·ªë gi√° tr·ªã pixel (RGB) - T·∫≠p Train')
        ax.set_xlabel('Gi√° tr·ªã pixel')
        ax.set_ylabel('T·∫ßn su·∫•t')
        ax.legend()
        st.pyplot(fig)
    except Exception as e:
        st.error(f"Error analyzing pixel distribution: {str(e)}")
        import traceback
    ax.legend()
